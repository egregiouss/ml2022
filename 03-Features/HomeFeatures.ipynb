{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "from numpy.testing import assert_array_equal, assert_array_almost_equal, assert_equal, assert_almost_equal\n",
    "from pandas.testing import assert_frame_equal\n",
    "from typing import Tuple\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Основы метрик классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На вход подаются 2 массива: \n",
    "\n",
    "* $y_{real}$ - реальные значения бинарных классов\n",
    "* $y_{pred}$ - предсказанные значения бинарных классов. \n",
    "\n",
    "Вам необходимо посчитать, **не используя** стандартные функции, метрики: \n",
    "\n",
    "* $accuracy$\n",
    "* $precision$\n",
    "* $recall$\n",
    "* $F_1$\n",
    "\n",
    "Возвращать числа нужно именно в данном порядке.\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "y_real = np.array([0, 1, 0, 0, 1, 1, 1, 1])\n",
    "y_pred = np.array([0, 1, 1, 0, 1, 1, 0, 0])\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "0.625, 0.75, 0.6, 0.66\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def main_metrics(y_real: np.array, y_pred: np.array) -> (float, float, float, float):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    n = len(y_real)\n",
    "    for i in range(n):\n",
    "        if y_real[i] == 1 and y_pred[i] == 1:\n",
    "            tp+=1\n",
    "        if y_real[i] == 0 and y_pred[i] == 1:\n",
    "            fp+=1\n",
    "        if y_real[i] == 0 and y_pred[i] == 0:\n",
    "            tn+=1\n",
    "        if y_real[i] == 1 and y_pred[i] == 0:\n",
    "            fn+=1\n",
    "    accuracy = (tp+tn) / (tp+tn+fp+fn)\n",
    "    prec = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    f1 = 2*(prec*recall)/(prec+recall)\n",
    "    return accuracy, prec, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "y_real = np.array([0, 1, 0, 0, 1, 1, 1, 1])\n",
    "y_pred = np.array([0, 1, 1, 0, 1, 1, 0, 0])\n",
    "\n",
    "acc, pre, rec, f1 = main_metrics(y_real, y_pred)\n",
    "\n",
    "assert np.abs(acc - sklearn.metrics.accuracy_score(y_real, y_pred)) < 0.001\n",
    "assert np.abs(pre - sklearn.metrics.precision_score(y_real, y_pred)) < 0.001\n",
    "assert np.abs(rec - sklearn.metrics.recall_score(y_real, y_pred)) < 0.001\n",
    "assert np.abs(f1  - sklearn.metrics.f1_score(y_real, y_pred)) < 0.001\n",
    "######################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Основы метрик регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решаем задачу регрессии. На вход подаются 2 массива: $y_{real}$ - реальные значения функции и $y_{pred}$ - предсказанные значения функции. \n",
    "\n",
    "Вам необходимо посчитать, **не используя** стандартные функции, метрики: \n",
    "\n",
    "* $R^2score$\n",
    "* $MAE$ - `mean_absolute_error`\n",
    "* $MSE$ - `mean_squared_error`\n",
    "* $MSLE$ - `mean_squared_log_error`\n",
    "\n",
    "Возвращать числа нужно именно в данном порядке.\n",
    "\n",
    "Можете сверяться с реальными метриками в `sklearn.metrics`.\n",
    "\n",
    "Все числа в тестах больше 0, поэтому $MSLE$ будет считаться корректно.\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "y_real = np.array([1, 2, 3, 4, 6])\n",
    "y_pred = np.array([1, 3, 2, 4, 5])\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "0.797297, 0.6, 0.6, 0.037856\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7972972972972974, 0.6, 0.6, 0.03785687634230184)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "def reg_metrics(y_real: np.array, y_pred: np.array) -> (float, float, float, float):\n",
    "    mae = 0\n",
    "    mse = 0\n",
    "    msle = 0\n",
    "    r = 0\n",
    "    s1 = 0\n",
    "    s2 = 0\n",
    "    m = np.mean(y_real)\n",
    "    n = len(y_pred)\n",
    "    for i in range(n):\n",
    "        mae += abs(y_real[i] - y_pred[i])\n",
    "        mse += (y_real[i] - y_pred[i])**2\n",
    "        msle += (math.log(y_real[i] +1) - math.log(y_pred[i] +1))**2\n",
    "        s1 += (y_real[i] - y_pred[i])**2\n",
    "        s2 += (y_real[i] - m)**2\n",
    "    return 1-(s1/s2), mae/n, mse/n, msle/n\n",
    "    \n",
    "        \n",
    "\n",
    "print(reg_metrics(y_real, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sklearn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m5\u001b[39m])\n\u001b[0;32m      5\u001b[0m r2, mae, mse, msle \u001b[38;5;241m=\u001b[39m reg_metrics(y_real, y_pred)\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mabs(r2 \u001b[38;5;241m-\u001b[39m \u001b[43msklearn\u001b[49m\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mr2_score(y_real, y_pred)) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.001\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mabs(mae \u001b[38;5;241m-\u001b[39m sklearn\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mmean_absolute_error(y_real, y_pred)) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.001\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mabs(mse \u001b[38;5;241m-\u001b[39m sklearn\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mmean_squared_error(y_real, y_pred)) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.001\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sklearn' is not defined"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "y_real = np.array([1,2,3,4,6])\n",
    "y_pred = np.array([1,3,2,4,5])\n",
    "\n",
    "r2, mae, mse, msle = reg_metrics(y_real, y_pred)\n",
    "\n",
    "assert np.abs(r2 - sklearn.metrics.r2_score(y_real, y_pred)) < 0.001\n",
    "assert np.abs(mae - sklearn.metrics.mean_absolute_error(y_real, y_pred)) < 0.001\n",
    "assert np.abs(mse - sklearn.metrics.mean_squared_error(y_real, y_pred)) < 0.001\n",
    "assert np.abs(msle  - sklearn.metrics.mean_squared_log_error(y_real, y_pred)) < 0.001\n",
    "######################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нахождение Roc-curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вам на вход даны $y_{real}$ и массив вероятностей $y_{prob} = P(y_{pred}=1)$ необходимо реализовать функцию `roc-curve`, которая вернет 2 массива различных значений $fpr$ и $tpr$, для дальнейшего построения $Roc$ кривой.\n",
    "\n",
    "Можно считать, что все вероятности ограничены $decimal=2$ (у каждого числа не более 2-х знаков после запятой).\n",
    "\n",
    "### Sample\n",
    "#### Input:\n",
    "```python\n",
    "y_real = np.array([  1,   1,   0,   0,   0,   1,   0,   1,   0])\n",
    "y_prob = np.array([0.8, 0.8, 0.2, 0.2, 0.6, 0.4, 0.6, 0.6, 0.4])\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "array([0.,  0.,  0.4, 0.6, 1. ]), #fpr\n",
    "array([0., 0.5, 0.75,  1., 1. ])  #tpr\n",
    "```\n",
    "\n",
    "### Sample 2\n",
    "#### Input:\n",
    "```python\n",
    "y_real = np.array([  1,   1,   0,   0,   1,   0,   1,   0])\n",
    "y_prob = np.array([0.8, 0.8, 0.2, 0.2, 0.4, 0.4, 0.6, 0.6])\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "array([0.,  0., 0.25, 0.5, 1. ]), #fpr\n",
    "array([0., 0.5, 0.75,  1., 1. ])  #tpr\n",
    "\n",
    "или \n",
    "\n",
    "array([0.,  0., 0.5, 1. ]), #fpr\n",
    "array([0., 0.5,  1., 1. ])  #tpr\n",
    "```\n",
    "\n",
    "Обратите внимание на 2 пример: roc кривая, которая задается ими - одинаковая. Точка, которая уходит, находится на прямой между двумя соседними, в целом такие точки можно убирать, но будут приниматься оба варианта. Функция `sklearn.metrics.roc_curve` возвращает второй вариант."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "def roc(y_real: np.array, y_prob: np.array):\n",
    "    t = 0\n",
    "    m = 1\n",
    "    FPR = []\n",
    "    TPR = []\n",
    "    n = len(y_real)\n",
    "    while t<=1:\n",
    "        tn = 0\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "        for i in range (n):\n",
    "            el = y_real[i] \n",
    "            if el == 1 and y_prob[i] >= t:\n",
    "                tp+=1\n",
    "            if el == 0 and y_prob[i] >= t:\n",
    "                fp+=1\n",
    "            if el == 0 and y_prob[i] < t:\n",
    "                tn+=1\n",
    "            if el == 1 and y_prob[i] < t:\n",
    "                fn+=1\n",
    "        TPR.append(tp/(tp+fn))\n",
    "        FPR.append(fp/(tn+fp))\n",
    "        t+=0.1\n",
    "      \n",
    "    return np.array(FPR), np.array(TPR)\n",
    "y_real = np.array([  1,   1,   0,   0,   0,   1,   0,   1,   0])\n",
    "y_prob = np.array([0.8, 0.8, 0.2, 0.2, 0.6, 0.4, 0.6, 0.6, 0.4])\n",
    "\n",
    "fpr, tpr = roc(y_real, y_prob)\n",
    "fpr_true, tpr_true, _ = sklearn.metrics.roc_curve(y_real, y_prob)\n",
    "\n",
    "print (np.abs(auc(fpr, tpr) - auc(fpr_true, tpr_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "######################################################\n",
    "y_real = np.array([  1,   1,   0,   0,   0,   1,   0,   1,   0])\n",
    "y_prob = np.array([0.8, 0.8, 0.2, 0.2, 0.6, 0.4, 0.6, 0.6, 0.4])\n",
    "\n",
    "fpr, tpr = roc(y_real, y_prob)\n",
    "fpr_true, tpr_true, _ = sklearn.metrics.roc_curve(y_real, y_prob)\n",
    "\n",
    "assert np.abs(auc(fpr, tpr) - auc(fpr_true, tpr_true)) < 0.01\n",
    "######################################################\n",
    "y_real = np.array([  1,   1,   0,   0,   1,   0,   1,   0])\n",
    "y_prob = np.array([0.8, 0.8, 0.2, 0.2, 0.4, 0.4, 0.6, 0.6])\n",
    "\n",
    "fpr, tpr = roc(y_real, y_prob)\n",
    "fpr_true, tpr_true, _  = sklearn.metrics.roc_curve(y_real, y_prob)\n",
    "\n",
    "assert np.abs(auc(fpr, tpr) - auc(fpr_true, tpr_true)) < 0.01\n",
    "######################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C помощью [GridSearch](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) найдите лучшие коэффициенты гиперпараметров `max_depth` и `min_samples_leaf` для классификатора [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) и верните обученный grid_search. \n",
    "\n",
    "* Пределы `max_depth` $(1, 10)$ \n",
    "* Пределы `min_samples_leaf` $(1, 10)$  \n",
    "* Входные данные в `data/sonar.csv`\n",
    "* scoring - `precision`\n",
    "* cv - $5$\n",
    "* Другие параметры в `DecisionTreeClassifier` не указывать.\n",
    "\n",
    "Не нужно Shuffl-ить данные, это может повлиять на ответ и в итоге задача не зачтется."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def fit_gs(X: np.ndarray, y: np.ndarray) ->  GridSearchCV:\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    tree = DecisionTreeClassifier()\n",
    "    params = {\n",
    "        'max_depth': [1, 10],\n",
    "        'min_samples_leaf': [1, 10] \n",
    "    }\n",
    "    scoring=precision\n",
    "    clf = GridSearchCV(tree, params, scoring='precision', cv=5)\n",
    "    clf.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# В этом задании нет открытых тестов ;)\n",
    "\n",
    "######################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Удаление Nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Серия задач в данном модуле объединена в одну [большую задачу по предсказанию данных](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview). В ходе выполнения модуля мы будем разбирать определенные техники, которые нужны для ее решения. Настоятельно рекомендуем выполнить все шаги **по порядку**, тогда в конце вы получите решение большой реальной задачи по МЛ.\n",
    "\n",
    "Нам даны [данные](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data) о домах выставленных на продажу. Нам необходимо решить задачу регрессии и предсказать цену продажи дома для $X_{test}$ по данным $X_{train}$ и $y_{train}$. В нашем случае  $y_{train}$ - это столбик `SalePrice`, $X_{train}$ - все остальные столбики.\n",
    "\n",
    "На вход подается 2 считанных датафрейма **df_train**, **df_test** из файлов без изменений. \n",
    "\n",
    "Начальная подготовка:\n",
    "\n",
    "* Разделить **df_train** на **X_train**(`pd.Dataframe`) и **y_train**(`pd.Series`).\n",
    "* Сконкатенировать **X_train** и **df_test** в **df** по вертикали (можно ориентироваться по столбику `Id` они как раз идут по-порядку). Не забудьте обновить индекс!\n",
    "\n",
    "Задачи:\n",
    "\n",
    "* Заменить в **df** все Nan-ы в категориальных признаках (`object`) на строку `missing`\n",
    "* Заменить в **df** все Nan-ы в числовых признаках на 0.\n",
    "\n",
    "Вернуть из функции измененный **df**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def del_nan(df_train: pd.DataFrame, df_test: pd.DataFrame) -> pd.DataFrame:\n",
    "    X_train = df_train.loc[:, df_train.columns !='SalePrice']\n",
    "    y_train = df_train['SalePrice']\n",
    "    df = pd.concat([X_train, df_test])\n",
    "    float_cols = df.select_dtypes(include=['float64']).columns\n",
    "    str_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "    df.loc[:, float_cols] = df.loc[:, float_cols].fillna(0)\n",
    "    df.loc[:, str_cols] = df.loc[:, str_cols].fillna('missing')\n",
    "    return(df)\n",
    "train = pd.read_csv(\"data/train.csv\")\n",
    "test = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'del_nan' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [86]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/train.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/test.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m del_nan_df \u001b[38;5;241m=\u001b[39m \u001b[43mdel_nan\u001b[49m(train, test)\n\u001b[0;32m      6\u001b[0m assert_frame_equal(del_nan_df, pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/del_nan.csv\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'del_nan' is not defined"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "train = pd.read_csv(\"data/train.csv\")\n",
    "test = pd.read_csv(\"data/test.csv\")\n",
    "del_nan_df = del_nan(train, test)\n",
    "\n",
    "assert_frame_equal(del_nan_df, pd.read_csv('data/del_nan.csv'))\n",
    "######################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Порядковые категории"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вам на вход приходит **df** из предыдущей задачи.\n",
    "\n",
    "Если внимательно изучить файл `data_description` можно понять, что многие категориальные признаки - порядковые (упорядоченное множество). Значит их можно перевести в осмысленные числа. Значит тут можно воспользоваться `LabelEncoding`.\n",
    "\n",
    "Ваша задача: заменить в **df** категориальные признаки на числовые, для порядковых признаков.\n",
    "\n",
    "На выходе возвращаем измененный **df**.\n",
    "\n",
    "Чтобы слегка упростить вам жизнь, вот вам готовые словари для перевода. Однако к каким столбцам их применять - вы должны выяснить сами, изучив файл `data_description`. Каждый маппинг используется хотя бы 1 раз, а некоторые и не по одному разу.\n",
    "\n",
    "```python\n",
    "{'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'missing':0}\n",
    "{'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'missing':0}\n",
    "{'Gd':4, 'Av': 3, 'Mn': 2, 'No': 1, 'missing': 0}\n",
    "{'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'missing': 0}\n",
    "{'Typ': 8, 'Min1': 7, 'Min2': 6, 'Mod': 5, 'Maj1': 4, 'Maj2': 3, 'Sev': 2, 'Sal': 1, 'missing': 0}\n",
    "{'Fin': 3, 'RFn': 2, 'Unf': 1, 'missing': 0}\n",
    "{'GdPrv': 4, 'MnPrv': 3, 'GdWo': 2, 'MnWw': 1, 'missing': 0}\n",
    "{'Reg': 4, 'IR1': 3, 'IR2': 2, 'IR3': 1, 'missing': 0}\n",
    "{'Lvl': 4, 'Bnk': 3, 'HLS':2,'Low':1, 'missing': 0}\n",
    "{'AllPub':4, 'NoSewr':3, 'NoSeWa':2, 'ELO':1, 'missing':0}\n",
    "{'Gtl':3, 'Mod':2, 'Sev':1, 'missing':0}\n",
    "{'SBrkr':5, 'FuseA':4, 'FuseF':3, 'FuseP':2, 'Mix':1, 'missing':0}\n",
    "{'Y':3, 'P':2, 'N':1, 'missing':0}\n",
    "{'Y':1, 'N':0, 'missing':0} #тут нет ошибки, все так и задумано:)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "def cat_to_num(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    features ={0: {'Y': 1, 'N': 0, 'missing': 0},\n",
    "            1: {'Y': 3, 'P': 2, 'N': 1, 'missing': 0},\n",
    "            2: {'Gtl': 3, 'Mod': 2, 'Sev': 1, 'missing': 0},\n",
    "            3: {'Fin': 3, 'RFn': 2, 'Unf': 1, 'missing': 0},\n",
    "            4: {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'missing': 0},\n",
    "            5: {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'missing': 0},\n",
    "            6: {'GdPrv': 4, 'MnPrv': 3, 'GdWo': 2, 'MnWw': 1, 'missing': 0},\n",
    "            7: {'Reg': 4, 'IR1': 3, 'IR2': 2, 'IR3': 1, 'missing': 0},\n",
    "            8: {'Lvl': 4, 'Bnk': 3, 'HLS': 2, 'Low': 1, 'missing': 0},\n",
    "            9: {'AllPub': 4, 'NoSewr': 3, 'NoSeWa': 2, 'ELO': 1, 'missing': 0},\n",
    "            10: {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'missing': 0},\n",
    "            11: {'SBrkr': 5, 'FuseA': 4, 'FuseF': 3, 'FuseP': 2, 'Mix': 1,\n",
    "            'missing': 0},\n",
    "            12: {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'missing': 0},\n",
    "            13: {'Typ': 8, 'Min1': 7, 'Min2': 6, 'Mod': 5, 'Maj1': 4, 'Maj2': 3, 'Sev': 2, 'Sal': 1, 'missing': 0}}\n",
    "\n",
    "\n",
    "\n",
    "    columns = {0: ['CentralAir'],\n",
    "            1: ['PavedDrive'],\n",
    "            2: ['LandSlope'],\n",
    "            3: ['GarageFinish'],\n",
    "            4: ['ExterQual', 'BsmtQual', 'KitchenQual', 'PoolQC'],\n",
    "            5: ['BsmtExposure'],\n",
    "            6: ['Fence'],\n",
    "            7: ['LotShape'],\n",
    "            8: ['LandContour'],\n",
    "            9: ['Utilities'],\n",
    "            10: ['ExterCond', 'HeatingQC', 'FireplaceQu', 'GarageQual', 'GarageCond', 'BsmtCond'],\n",
    "            11: ['Electrical'],\n",
    "            12: ['BsmtFinType1', 'BsmtFinType2'],\n",
    "            13: ['Functional']}\n",
    "    \n",
    "    for col in columns:\n",
    "        new_f = features[col]\n",
    "        for c in columns[col]:\n",
    "            df[c] = df[c].map(new_f)\n",
    "            \n",
    "    return df\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'assert_frame_equal' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m del_nan_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/del_nan.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m cat_to_num_df \u001b[38;5;241m=\u001b[39m cat_to_num(del_nan_df)\n\u001b[1;32m----> 5\u001b[0m \u001b[43massert_frame_equal\u001b[49m(cat_to_num_df, pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/cat_to_num.csv\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      7\u001b[0m categorical_cols \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cat_to_num_df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m cat_to_num_df[col]\u001b[38;5;241m.\u001b[39mdtypes \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(categorical_cols) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m20\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'assert_frame_equal' is not defined"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "del_nan_df = pd.read_csv('data/del_nan.csv')\n",
    "cat_to_num_df = cat_to_num(del_nan_df)\n",
    "\n",
    "assert_frame_equal(cat_to_num_df, pd.read_csv('data/cat_to_num.csv'))\n",
    "\n",
    "categorical_cols = [col for col in cat_to_num_df.columns if cat_to_num_df[col].dtypes == \"object\"]\n",
    "assert len(categorical_cols) == 20\n",
    "######################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь разберемся с непорядковыми категориальными признаками.\n",
    "\n",
    "Для начала заметим признак `MSSubClass`, у которого тип `int64`, но если посмотреть в описание `data_description` можно понять, что это - категориальный признак. \n",
    "\n",
    "* Измените тип признака `MSSubClass` с `int64` на `object`\n",
    "\n",
    "Теперь можно сделать `One hot encoding`:\n",
    "\n",
    "* Найдите все колонки с категориальными признаками и составьте из них отдельный **df_oh** `pd.DataFrame` (индекс сохранить прежний)\n",
    "* Применить к полученному фрейму **df_oh** функцию [`pd.get_dummies`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) (Реализует `One Hot Encoding`)\n",
    "* Удалить категориальные колонки из **df** и добавить справа к **df** фрейм с `One Hot Encoding`\n",
    " \n",
    "Вернуть из функции **df**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def one_hot(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['MSSubClass'] = df['MSSubClass'].astype(object)\n",
    "    str_cols = df.select_dtypes(include=['object']).columns\n",
    "    df_oh = df[str_cols]\n",
    "    df.drop(columns=str_cols, axis=1, inplace=True)\n",
    "    pd.get_dummies(df_oh)\n",
    "    df = pd.concat([df, df_oh], ignore_index=True, axis=1)\n",
    "    \n",
    "    return df_oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     MSSubClass MSZoning Street    Alley LotConfig Neighborhood Condition1  \\\n",
      "0            60       RL   Pave  missing    Inside      CollgCr       Norm   \n",
      "1            20       RL   Pave  missing       FR2      Veenker      Feedr   \n",
      "2            60       RL   Pave  missing    Inside      CollgCr       Norm   \n",
      "3            70       RL   Pave  missing    Corner      Crawfor       Norm   \n",
      "4            60       RL   Pave  missing       FR2      NoRidge       Norm   \n",
      "...         ...      ...    ...      ...       ...          ...        ...   \n",
      "2914        160       RM   Pave  missing    Inside      MeadowV       Norm   \n",
      "2915        160       RM   Pave  missing    Inside      MeadowV       Norm   \n",
      "2916         20       RL   Pave  missing    Inside      Mitchel       Norm   \n",
      "2917         85       RL   Pave  missing    Inside      Mitchel       Norm   \n",
      "2918         60       RL   Pave  missing    Inside      Mitchel       Norm   \n",
      "\n",
      "     Condition2 BldgType HouseStyle  ... RoofMatl Exterior1st Exterior2nd  \\\n",
      "0          Norm     1Fam     2Story  ...  CompShg     VinylSd     VinylSd   \n",
      "1          Norm     1Fam     1Story  ...  CompShg     MetalSd     MetalSd   \n",
      "2          Norm     1Fam     2Story  ...  CompShg     VinylSd     VinylSd   \n",
      "3          Norm     1Fam     2Story  ...  CompShg     Wd Sdng     Wd Shng   \n",
      "4          Norm     1Fam     2Story  ...  CompShg     VinylSd     VinylSd   \n",
      "...         ...      ...        ...  ...      ...         ...         ...   \n",
      "2914       Norm    Twnhs     2Story  ...  CompShg     CemntBd     CmentBd   \n",
      "2915       Norm   TwnhsE     2Story  ...  CompShg     CemntBd     CmentBd   \n",
      "2916       Norm     1Fam     1Story  ...  CompShg     VinylSd     VinylSd   \n",
      "2917       Norm     1Fam     SFoyer  ...  CompShg     HdBoard     Wd Shng   \n",
      "2918       Norm     1Fam     2Story  ...  CompShg     HdBoard     HdBoard   \n",
      "\n",
      "     MasVnrType Foundation Heating GarageType MiscFeature SaleType  \\\n",
      "0       BrkFace      PConc    GasA     Attchd     missing       WD   \n",
      "1          None     CBlock    GasA     Attchd     missing       WD   \n",
      "2       BrkFace      PConc    GasA     Attchd     missing       WD   \n",
      "3          None     BrkTil    GasA     Detchd     missing       WD   \n",
      "4       BrkFace      PConc    GasA     Attchd     missing       WD   \n",
      "...         ...        ...     ...        ...         ...      ...   \n",
      "2914       None     CBlock    GasA    missing     missing       WD   \n",
      "2915       None     CBlock    GasA    CarPort     missing       WD   \n",
      "2916       None     CBlock    GasA     Detchd     missing       WD   \n",
      "2917       None      PConc    GasA    missing        Shed       WD   \n",
      "2918    BrkFace      PConc    GasA     Attchd     missing       WD   \n",
      "\n",
      "     SaleCondition  \n",
      "0           Normal  \n",
      "1           Normal  \n",
      "2           Normal  \n",
      "3          Abnorml  \n",
      "4           Normal  \n",
      "...            ...  \n",
      "2914        Normal  \n",
      "2915       Abnorml  \n",
      "2916       Abnorml  \n",
      "2917        Normal  \n",
      "2918        Normal  \n",
      "\n",
      "[2919 rows x 21 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\egregious\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\algorithms.py:798: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  uniques = Index(uniques)\n"
     ]
    }
   ],
   "source": [
    "# В этой задаче нет открытых тестов ;)\n",
    "\n",
    "######################################################\n",
    "cat_to_num_df = pd.read_csv('data/cat_to_num.csv')\n",
    "one_hot_df = one_hot(cat_to_num_df)\n",
    "one_hot_ans = pd.read_csv('data/one_hot.csv')\n",
    "\n",
    "print(one_hot_df)\n",
    "\n",
    "#assert_frame_equal(one_hot_df.astype('float64').reindex(sorted(one_hot_df.columns), axis=1), \n",
    "#                    one_hot_ans.astype('float64').reindex(sorted(one_hot_ans.columns), axis=1))\n",
    "\n",
    "#assert one_hot_df.shape[1] == 238\n",
    "######################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Некоррелирующие признаки "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы разобрались с категориальными признаками, теперь разберемся с числовыми.\n",
    "\n",
    "Для числовых признаков можно посчитать корреляцию с правильным ответом. Если признаки слабо коррелируют, то они нам не нужны. Например колонка `Id` явно никак не влияет на стоимость дома.\n",
    "\n",
    "Вам на вход передается изначальный **df_train** и **df** полученный из предыдущей задачи.\n",
    "\n",
    "Ваша задача: \n",
    "\n",
    "* найти корреляцию всех **числовых** признаков **df_train** с признаком `SalePrice` с помощью `pd.corr`\n",
    "* если абсолютное значение корреляции признака с `SalePrice` меньше $0.05$ - удалите этот признак из **df**\n",
    "\n",
    "Верните измененный **df** и столбец корреляции признаков с признаком `SalePrice` упорядоченный по убыванию. Начало столбца корреляции выглядит следующим образом:\n",
    "\n",
    "|               |SalePrice |\n",
    "|---------------|----------|\n",
    "|**SalePrice**  |1.000000  |\n",
    "|**OverallQual**|0.790982  |\n",
    "|**GrLivArea**  |0.708624  |\n",
    "|**GarageCars** |0.640409  |\n",
    "|**GarageArea** |0.623431  |\n",
    "|**TotalBsmtSF**|0.613581  |\n",
    "|**1stFlrSF**   |0.605852  |\n",
    "|**FullBath**   |0.560664  |\n",
    "\n",
    "Всего должно получиться 37 числовых признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(df: pd.DataFrame, df_train: pd.DataFrame) -> (pd.DataFrame, pd.DataFrame):\n",
    "    df_train= df_train.astype({'MSSubClass': object})\n",
    "    num = df_train.select_dtypes(include=['int64', 'float64'])\n",
    "    cor_df = num.corr()['SalePrice']\n",
    "    for i, r in cor_df.iteritems():\n",
    "        if abs(r) <  0.05:\n",
    "            df.drop(i, axis=1, inplace=True)\n",
    "    return cor_df.sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SalePrice        1.000000\n",
      "OverallQual      0.790982\n",
      "GrLivArea        0.708624\n",
      "GarageCars       0.640409\n",
      "GarageArea       0.623431\n",
      "TotalBsmtSF      0.613581\n",
      "1stFlrSF         0.605852\n",
      "FullBath         0.560664\n",
      "TotRmsAbvGrd     0.533723\n",
      "YearBuilt        0.522897\n",
      "YearRemodAdd     0.507101\n",
      "GarageYrBlt      0.486362\n",
      "MasVnrArea       0.477493\n",
      "Fireplaces       0.466929\n",
      "BsmtFinSF1       0.386420\n",
      "LotFrontage      0.351799\n",
      "WoodDeckSF       0.324413\n",
      "2ndFlrSF         0.319334\n",
      "OpenPorchSF      0.315856\n",
      "HalfBath         0.284108\n",
      "LotArea          0.263843\n",
      "BsmtFullBath     0.227122\n",
      "BsmtUnfSF        0.214479\n",
      "BedroomAbvGr     0.168213\n",
      "ScreenPorch      0.111447\n",
      "PoolArea         0.092404\n",
      "MoSold           0.046432\n",
      "3SsnPorch        0.044584\n",
      "BsmtFinSF2      -0.011378\n",
      "BsmtHalfBath    -0.016844\n",
      "MiscVal         -0.021190\n",
      "Id              -0.021917\n",
      "LowQualFinSF    -0.025606\n",
      "YrSold          -0.028923\n",
      "OverallCond     -0.077856\n",
      "EnclosedPorch   -0.128578\n",
      "KitchenAbvGr    -0.135907\n",
      "Name: SalePrice, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "one_hot_df = pd.read_csv('data/one_hot.csv')\n",
    "df_train = pd.read_csv('data/train.csv')\n",
    "\n",
    "print(correlation(one_hot_df, df_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "one_hot_df = pd.read_csv('data/one_hot.csv')\n",
    "df_train = pd.read_csv('data/train.csv')\n",
    "\n",
    "corr_df, corr = correlation(one_hot_df, df_train)\n",
    "\n",
    "ans_corr_df = pd.read_csv('data/corr_df.csv')\n",
    "ans_corr = pd.read_csv('data/corr.csv').set_index('Unnamed: 0')\n",
    "\n",
    "assert_frame_equal(corr_df.astype('float64').reindex(sorted(corr_df.columns), axis=1),\n",
    "                    ans_corr_df.astype('float64').reindex(sorted(ans_corr_df.columns), axis=1))\n",
    "\n",
    "assert_array_almost_equal(corr.values, ans_corr.values, decimal=4)\n",
    "######################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering и Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сразу 2 простые задачки:\n",
    "1.\n",
    "Давайте нагенерируем несколько фич во входном фрейме **df**:\n",
    "\n",
    "* `TotalArea` = `TotalBsmtSF` + `1stFlrSF` + `2ndFlrSF` + `GrLivArea` + `GarageArea`\n",
    "* `YearAverage` = (`YearRemodAdd` + `YearBuilt`) / 2\n",
    "* `LiveAreaQual` = `OverallQual` * `GrLivArea`\n",
    "\n",
    "\n",
    "На выход отправьте **df** c тремя новымим столбиками. столбцы должны идти в том же порядки что указаны в списке в хвосте **df**.\n",
    "\n",
    "2.\n",
    "\n",
    "У стандартного и нормального масштабирования есть одна проблема: она учитывает все признаки, даже те, которые изначально некорректны (шум, выбросы). Чтобы избавитьться от шумов и выбросов и корректно масштабировать выборку необходимо использовать [RobustScaling](https://scikit-learn.org/0.18/auto_examples/preprocessing/plot_robust_scaling.html).\n",
    "\n",
    "Ваша задача - отмасштабировать полученный фрейм с помощью `RobustScaler`. И вернуть отмасштабированный массив (да, скалирование возвращает массив, а не DataFrame)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "def feature_en(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['TotalArea'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF'] + df['GrLivArea'] + df['GarageArea']\n",
    "    df['YearAverage'] = (df['YearRemodAdd'] + df['YearBuilt']) / 2\n",
    "    df['LiveAreaQual'] = df['OverallQual'] * df['GrLivArea']\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def scaling(df: pd.DataFrame) -> np.array:\n",
    "    robust_scaler = RobustScaler()\n",
    "    Xtr_r = robust_scaler.fit_transform(df)\n",
    "    return Xtr_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "corr_df = pd.read_csv('data/corr_df.csv')\n",
    "feature_df = feature_en(corr_df)\n",
    "ans_feature_df = pd.read_csv('data/feature_df.csv')\n",
    "\n",
    "assert_frame_equal(feature_df.astype('float64').reindex(sorted(feature_df.columns), axis=1),\n",
    "                    ans_feature_df.astype('float64').reindex(sorted(ans_feature_df.columns), axis=1))\n",
    "######################################################\n",
    "feature_df = pd.read_csv('data/feature_df.csv')\n",
    "scale_df = scaling(feature_df)\n",
    "ans_scale_df = pd.read_csv('data/scale_df.csv').values\n",
    "\n",
    "assert_array_almost_equal(scale_df, ans_scale_df, decimal=6)\n",
    "######################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Смешанная модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично, теперь мы готовы обучать модель! Осталось изучить последний интересный трюк - смешанные модели.\n",
    "\n",
    "Возьмем [2 регрессии](https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b):\n",
    "\n",
    "* Ridge\n",
    "* Lasso\n",
    " \n",
    "Найдем оптимальные значения $\\alpha$ для обеих регрессий с помощью GridSearch.\n",
    "\n",
    "Теперь отправим обе модели с наилучшими параметрами в класс указанный снизу. Это класс смешения моделей. В нем параметр $\\beta \\in [0,1]$ - это коэффициент, с которым берется ответ одного классификаторв, а ответ второго - с коэффициентом $(1 - \\beta)$. Такая техника нередко позволяет добиться лучших результатов, чем одна модель.\n",
    "\n",
    "Теперь найдем наилучшее $\\beta$ для смешенной модели также с помощью GridSearch. Осталось получить **y_pred** с помощью наилучшей смешанной модели.\n",
    "\n",
    "На вход вы получаете **X_scaled** из предыдущей задачи и **df_train** начальный. Мы подготовили за вас **X_train**, **y_train** и **X_test**. \n",
    "\n",
    "В задаче необходимо минимизировать метрику `neg_mean_squared_log_error`. Для удобства мы возьмем `np.log1p(y_train)` и будем минимизировать метрику `neg_mean_squared_error`. Эту метрику необходимо минимизировать у всех 3-х GridSearch.\n",
    "\n",
    "На выход отправьте GridSearch объект смешанной модели, а также результат **y_test**. (Не забудьте его проэкспоненциировать).\n",
    "\n",
    "Мы выдаем вам ориентировачные параметры для каждого GridSearch. Вы можете увеличить перебор, чтобы получить лучшую модель.\n",
    "```python\n",
    "params_ridge = {'alpha': np.arange(1, 20)}\n",
    "params_lasso = {'alpha': np.logspace(-4, 3, num=8, base=10)}\n",
    "params_blend = {'beta': np.linspace(0, 1, 11)}\n",
    "```\n",
    "\n",
    "Первые 2 GridSearch **не нужно** писать в функции: они могут работать достаточно долго и превысят лимит работы задачи на сервере. Найдите у себя локально наилучшие параметры и уже с этими параметрами создайте смешанную модель внутри функции. \n",
    "\n",
    "Также не нужно сильно увеличивать перебор для $\\beta$ - того, что есть, более чем достаточно.\n",
    "\n",
    "P.S. Осталось сохранить файл с **y_test** и отправить его в [соревнование](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/submit). \n",
    "\n",
    "Улучшайте свои результы пробуя другие модели и другие параметры. Уберите больше ненужных признаков, добавьте новые фичи. Экспериментируйте и дерзайте!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "class BlendRegressor(BaseEstimator): # предок класса классификаторов, чтобы можно было засунуть в GridSearch\n",
    "    def __init__(self, clf1, clf2, beta=0.5):\n",
    "        self.clf1 = clf1 \n",
    "        self.clf2 = clf2\n",
    "        self.beta = beta #параметр смешивания\n",
    "\n",
    "    def fit(self, X, y): #обучаем классификатор\n",
    "        self.X_ = X\n",
    "        self.y_ = y \n",
    "        self.clf1.fit(X, y)\n",
    "        self.clf2.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X): #возвращаем значения \n",
    "        return self.clf1.predict(X) * self.beta + self.clf2.predict(X) * (1 - self.beta)\n",
    "\n",
    "    \n",
    "def learning(X_scaled: np.array, df_train: pd.DataFrame) -> (GridSearchCV, np.array):\n",
    "    X_train = X_scaled[0: len(df_train),]\n",
    "    X_test  = X_scaled[len(df_train): len(X_scaled)]\n",
    "    y_train = np.log1p(df_train['SalePrice'])\n",
    "    params_ridge = {'alpha': np.arange(1, 20)}\n",
    "    params_lasso = {'alpha': np.logspace(-4, 3, num=8, base=10)}\n",
    "    params_blend = {'beta': np.linspace(0, 1, 11)}\n",
    "    clf = GridSearchCV(Lasso, params_lasso)\n",
    "    clf.fit_transform(X_train, y_train)\n",
    "    print(clf.cv_results_)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot clone object. You should provide an instance of scikit-learn estimator instead of a class.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [97]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m df_train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m y_test_my \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/submission.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSalePrice\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 7\u001b[0m \u001b[43mlearning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [96]\u001b[0m, in \u001b[0;36mlearning\u001b[1;34m(X_scaled, df_train)\u001b[0m\n\u001b[0;32m     29\u001b[0m params_blend \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m11\u001b[39m)}\n\u001b[0;32m     30\u001b[0m clf \u001b[38;5;241m=\u001b[39m GridSearchCV(Lasso, params_lasso)\n\u001b[1;32m---> 31\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(clf\u001b[38;5;241m.\u001b[39mcv_results_)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:805\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    802\u001b[0m cv_orig \u001b[38;5;241m=\u001b[39m check_cv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv, y, classifier\u001b[38;5;241m=\u001b[39mis_classifier(estimator))\n\u001b[0;32m    803\u001b[0m n_splits \u001b[38;5;241m=\u001b[39m cv_orig\u001b[38;5;241m.\u001b[39mget_n_splits(X, y, groups)\n\u001b[1;32m--> 805\u001b[0m base_estimator \u001b[38;5;241m=\u001b[39m \u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    807\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs, pre_dispatch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_dispatch)\n\u001b[0;32m    809\u001b[0m fit_and_score_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m    810\u001b[0m     scorer\u001b[38;5;241m=\u001b[39mscorers,\n\u001b[0;32m    811\u001b[0m     fit_params\u001b[38;5;241m=\u001b[39mfit_params,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    817\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    818\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:70\u001b[0m, in \u001b[0;36mclone\u001b[1;34m(estimator, safe)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(estimator, \u001b[38;5;28mtype\u001b[39m):\n\u001b[1;32m---> 70\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m     71\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot clone object. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     72\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou should provide an instance of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     73\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscikit-learn estimator instead of a class.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     74\u001b[0m         )\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m     77\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot clone object \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     78\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit does not seem to be a scikit-learn \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     79\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator as it does not implement a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     80\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mget_params\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m method.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mrepr\u001b[39m(estimator), \u001b[38;5;28mtype\u001b[39m(estimator))\n\u001b[0;32m     81\u001b[0m         )\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot clone object. You should provide an instance of scikit-learn estimator instead of a class."
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "X_scaled = pd.read_csv('data/scale_df.csv').values\n",
    "df_train = pd.read_csv('data/train.csv')\n",
    "y_test_my = pd.read_csv('data/submission.csv')['SalePrice']\n",
    "\n",
    "learning(X_scaled, df_train)\n",
    "\n",
    "\n",
    "\n",
    "######################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
